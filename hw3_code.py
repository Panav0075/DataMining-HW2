# -*- coding: utf-8 -*-
"""HW3_code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tvc_YHvgIQNKeEEsY8cZmkEIbGAs5eTT
"""

import numpy as np, pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances

# load data
X = pd.read_csv("data.csv", header=None).values.astype(float)
y = pd.read_csv("label.csv", header=None).values.ravel()
K = len(np.unique(y))          # number of classes = 10
X = X / 255.0                  # scale pixels to [0,1] for all metrics

# 1) Euclidean K-means
km_euc = KMeans(n_clusters=K, random_state=0, n_init=5, max_iter=50)
km_euc.fit(X)
sse_euc = km_euc.inertia_      # SSE for Euclidean
print("SSE (Euclidean):", sse_euc)

# generic K-means using pairwise distances
def kmeans_custom(X, K, metric, iters=20, seed=0):
    rng = np.random.RandomState(seed)
    cent = X[rng.choice(len(X), K, replace=False)]
    for _ in range(iters):
        D = pairwise_distances(X, cent, metric=metric)
        lbl = D.argmin(axis=1)
        new_cent = np.vstack([X[lbl == k].mean(axis=0) for k in range(K)])
        if np.allclose(new_cent, cent): break
        cent = new_cent
    D = pairwise_distances(X, cent, metric=metric)
    return (D.min(axis=1) ** 2).sum()

# 2) Cosine K-means
sse_cos = kmeans_custom(X, K, metric="cosine")
print("SSE (Cosine):", sse_cos)

# 3) Generalized Jaccard distance
def gen_jaccard(u, v):
    mn = np.minimum(u, v).sum()
    mx = np.maximum(u, v).sum()
    return 1.0 - (mn / mx if mx > 0 else 0.0)

sse_jac = kmeans_custom(X, K, metric=gen_jaccard)
print("SSE (Generalized Jaccard):", sse_jac)

# === Task-1 Q2: cluster â†’ label mapping and accuracy ===

from collections import Counter

# helper: run K-means with given metric and return cluster labels
def kmeans_labels(X, K, metric, iters=20, seed=0):
    rng = np.random.RandomState(seed)
    cent = X[rng.choice(len(X), K, replace=False)]
    for _ in range(iters):
        D = pairwise_distances(X, cent, metric=metric)
        lbl = D.argmin(axis=1)
        cent = np.vstack([X[lbl == k].mean(axis=0) for k in range(K)])
    return lbl

# labels for each metric
labels_euc = km_euc.labels_                  # from sklearn KMeans (Euclidean)
labels_cos = kmeans_labels(X, K, "cosine")
labels_jac = kmeans_labels(X, K, gen_jaccard)

# majority-vote labeling + accuracy
def majority_vote_acc(cluster_labels, true_y, K):
    mapping = {}
    for k in range(K):
        idx = (cluster_labels == k)
        if not np.any(idx):
            continue
        mapping[k] = Counter(true_y[idx]).most_common(1)[0][0]
    preds = np.array([mapping.get(c, -1) for c in cluster_labels])
    return (preds == true_y).mean()

acc_euc = majority_vote_acc(labels_euc, y, K)
acc_cos = majority_vote_acc(labels_cos, y, K)
acc_jac = majority_vote_acc(labels_jac, y, K)

print(f"Accuracy (Euclidean): {acc_euc:.4f}")
print(f"Accuracy (Cosine):    {acc_cos:.4f}")
print(f"Accuracy (Jaccard):   {acc_jac:.4f}")

# === Task-1 Q3: K-means with custom stopping criteria ===

def kmeans_with_stop(X, K, metric, max_iter=500, seed=0):
    rng = np.random.RandomState(seed)
    cent = X[rng.choice(len(X), K, replace=False)]
    prev_sse = np.inf

    for it in range(max_iter):
        # assign points
        D = pairwise_distances(X, cent, metric=metric)
        labels = D.argmin(axis=1)

        # recompute centroids
        new_cent = np.vstack([X[labels == k].mean(axis=0) for k in range(K)])

        # stop if centroids do not move
        if np.allclose(new_cent, cent):
            D = pairwise_distances(X, new_cent, metric=metric)
            sse = (D.min(axis=1) ** 2).sum()
            print(f"{metric}: stop (no centroid change) at iter {it+1}, SSE={sse:.4f}")
            return new_cent, labels, sse

        cent = new_cent

        # compute SSE and check increase
        sse = (D.min(axis=1) ** 2).sum()
        if sse > prev_sse:
            print(f"{metric}: stop (SSE increased) at iter {it+1}, SSE={prev_sse:.4f}")
            return cent, labels, prev_sse

        prev_sse = sse

    print(f"{metric}: stop (max_iter reached), SSE={prev_sse:.4f}")
    return cent, labels, prev_sse


# run for Euclidean, Cosine, Generalized Jaccard
cent_euc2, lab_euc2, sse_euc2 = kmeans_with_stop(X, K, "euclidean")
cent_cos2, lab_cos2, sse_cos2 = kmeans_with_stop(X, K, "cosine")
cent_jac2, lab_jac2, sse_jac2 = kmeans_with_stop(X, K, gen_jaccard)

# === Task-1 Q4: SSE under three separate stopping conditions ===

def kmeans_trajectory(X, K, metric, max_iter=100, seed=0):
    rng = np.random.RandomState(seed)
    cent = X[rng.choice(len(X), K, replace=False)]
    prev_sse = None
    sse_nocc = None   # SSE when centroids stop changing
    sse_inc = None    # SSE just before first SSE increase

    for it in range(max_iter):
        D = pairwise_distances(X, cent, metric=metric)
        labels = D.argmin(axis=1)
        new_cent = np.vstack([X[labels == k].mean(axis=0) for k in range(K)])

        D_new = pairwise_distances(X, new_cent, metric=metric)
        sse = (D_new.min(axis=1) ** 2).sum()

        if sse_nocc is None and np.allclose(new_cent, cent):
            sse_nocc = sse

        if prev_sse is not None and sse_inc is None and sse > prev_sse:
            sse_inc = prev_sse

        cent, prev_sse = new_cent, sse

    sse_max = prev_sse
    return sse_nocc, sse_inc, sse_max


def print_q4(name, metric):
    s1, s2, s3 = kmeans_trajectory(X, K, metric, max_iter=100)
    print(f"{name}:")
    print(f"  SSE (no centroid change):   {s1}")
    print(f"  SSE (before SSE increase):  {s2}")
    print(f"  SSE (after max_iter=100):   {s3}\n")


print_q4("Euclidean", "euclidean")
print_q4("Cosine", "cosine")
print_q4("Jaccard", gen_jaccard)

# Task-2 a,b,c: read ratings_small.csv and evaluate 3 recommenders with 5-fold CV

import pandas as pd
from surprise import Dataset, Reader, NMF, KNNBasic
from surprise.model_selection import cross_validate

# a) read data: userId,movieId,rating,timestamp
df = pd.read_csv("ratings_small.csv")  # adjust path if needed

# build Surprise dataset (ratings are in [0.5, 5.0] for MovieLens small)
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(df[["userId", "movieId", "rating"]], reader)

# c) PMF (using NMF), User-based CF, Item-based CF with cosine similarity
algos = {
    "PMF (NMF)": NMF(verbose=False),
    "User-CF (cos)": KNNBasic(sim_options={"name": "cosine", "user_based": True}, verbose=False),
    "Item-CF (cos)": KNNBasic(sim_options={"name": "cosine", "user_based": False}, verbose=False),
}

for name, algo in algos.items():
    cv = cross_validate(algo, data, measures=["RMSE", "MAE"], cv=5, verbose=False)
    print(
        f"{name}: "
        f"RMSE={cv['test_rmse'].mean():.4f}, "
        f"MAE={cv['test_mae'].mean():.4f}"
    )

# Task-2(d): compare average performances of the three models

rows = []
for name, algo in algos.items():
    cv = cross_validate(algo, data, measures=["RMSE", "MAE"], cv=5, verbose=False)
    rows.append({
        "model": name,
        "rmse": cv["test_rmse"].mean(),
        "mae":  cv["test_mae"].mean()
    })

res = pd.DataFrame(rows).set_index("model")
print(res)

print("\nBest (lowest RMSE):", res["rmse"].idxmin())
print("Best (lowest MAE): ", res["mae"].idxmin())

import matplotlib.pyplot as plt
from surprise import KNNBasic
from surprise.model_selection import cross_validate

sims = ["cosine", "msd", "pearson"]
rows = []

for s in sims:
    for user_based in [True, False]:
        algo = KNNBasic(sim_options={"name": s, "user_based": user_based}, verbose=False)
        cv = cross_validate(algo, data, measures=["RMSE", "MAE"], cv=5, verbose=False)
        rows.append({
            "type": "User-CF" if user_based else "Item-CF",
            "sim": s,
            "rmse": cv["test_rmse"].mean(),
            "mae":  cv["test_mae"].mean()
        })

sim_res = pd.DataFrame(rows)
print(sim_res)

# plot RMSE
plt.figure(figsize=(6,4))
for t in ["User-CF", "Item-CF"]:
    subset = sim_res[sim_res["type"] == t]
    plt.plot(subset["sim"], subset["rmse"], marker="o", label=t)
plt.ylabel("RMSE")
plt.xlabel("Similarity")
plt.title("RMSE vs Similarity (User-CF vs Item-CF)")
plt.legend()
plt.grid(True)
plt.show()

# plot MAE
plt.figure(figsize=(6,4))
for t in ["User-CF", "Item-CF"]:
    subset = sim_res[sim_res["type"] == t]
    plt.plot(subset["sim"], subset["mae"], marker="o", label=t)
plt.ylabel("MAE")
plt.xlabel("Similarity")
plt.title("MAE vs Similarity (User-CF vs Item-CF)")
plt.legend()
plt.grid(True)
plt.show()

# Task-2(f): how number of neighbors (k) affects User-CF and Item-CF
# We fix similarity to 'msd' since it performed best in (e).

k_list = [5, 10, 20, 40, 80]
rows = []

for k in k_list:
    for user_based in [True, False]:
        algo = KNNBasic(
            k=k,
            sim_options={"name": "msd", "user_based": user_based},
            verbose=False,
        )
        cv = cross_validate(algo, data, measures=["RMSE", "MAE"], cv=5, verbose=False)
        rows.append({
            "type": "User-CF" if user_based else "Item-CF",
            "k": k,
            "rmse": cv["test_rmse"].mean(),
            "mae":  cv["test_mae"].mean(),
        })

k_res = pd.DataFrame(rows)
print(k_res)

# Plot RMSE vs k
plt.figure(figsize=(6,4))
for t in ["User-CF", "Item-CF"]:
    sub = k_res[k_res["type"] == t]
    plt.plot(sub["k"], sub["rmse"], marker="o", label=t)
plt.xlabel("Number of neighbors (k)")
plt.ylabel("RMSE")
plt.title("RMSE vs k (MSD similarity)")
plt.grid(True)
plt.legend()
plt.show()

# Plot MAE vs k
plt.figure(figsize=(6,4))
for t in ["User-CF", "Item-CF"]:
    sub = k_res[k_res["type"] == t]
    plt.plot(sub["k"], sub["mae"], marker="o", label=t)
plt.xlabel("Number of neighbors (k)")
plt.ylabel("MAE")
plt.title("MAE vs k (MSD similarity)")
plt.grid(True)
plt.legend()
plt.show()

# Task-2(g): find best K (lowest RMSE) for User-CF and Item-CF

best_user = k_res[k_res["type"] == "User-CF"].sort_values("rmse").iloc[0]
best_item = k_res[k_res["type"] == "Item-CF"].sort_values("rmse").iloc[0]

print(f"Best User-CF K: {best_user['k']}, RMSE={best_user['rmse']:.6f}")
print(f"Best Item-CF K: {best_item['k']}, RMSE={best_item['rmse']:.6f}")

